---
layout: custom 
title: MIDST Challenge @ SaTML 2025
permalink: /
---
<style>
p, ol, ul, li {
  color: #000000 !important
}
.main-content {
    margin-top: 30px; 
}
.main-content h1 {
    margin-top: 30px; 
}
</style>

# Announcing the Winners of the MIDST Challenge

The Vector Institute MIDST challenge (**M**embership **I**nference over **D**iffusion-models-based **S**ynthetic **T**abular data) will be hosted at the [3rd IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2025)](https://satml.org/). The competition was launched in December 2024, and final submissions were due on February 28th, 2025. We are excited to announce the winning submissions!

The goal of this challenge was to evaluate the resilience of the synthetic tabular data generated by diffusion models against black-box and white-box membership inference attacks. We sought a quantitative evaluation of the privacy gain of synthetic tabular data generated by diffusion models, with a specific focus on its resistance to membership inference attacks (MIAs). Given the heterogeneity and complexity of tabular data, we explored multiple target models for MIAs, including diffusion models for single tables of mixed data type types and multi-relational tables with interconnected constraints. We expected the development of novel black-box and white-box MIAs tailored to these target diffusion models as a key outcome, enabling a comprehensive evaluation of their privacy efficacy.
The following is a link to the GitHub repository: [link](https://github.com/VectorInstitute/MIDST)

* [Challenge Tasks](#challenge-tasks)
* [Evaluation Criteria](#evaluation-criteria)
* [Accessibility](#accessibility)
* [Transparency](#Transparency)
* [Result](#result)
* [Important Dates](#important-dates)
* [Terms and Conditions](#terms-and-conditions)
* [Getting Started](#getting-started)
* 
* [Event Organizers](#event-organizers)
* [Event Sponsors](#event-sponsors)
* [Frequently Asked Questions](#event-sponsors)
* [Acknowledgements](#acknowledgements)
* [Contact](#contact)

## Challenge Tasks

This challenge was composed of four different tasks, each associated with a separate category. The categories were defined based on the access to the generative models and the type of the tabular data as follows: 
Access to the models: black-box, Data: single table 
Access to the models: white-box, Data: single table 
Access to the models: black-box, Data: multi-table 
Access to the models: white-box, Data: multi-table

To facilitate participation in MIDST, we developed shadow models for both single table and multi-table tasks. The shadow models were the same for black-box and white-box tasks. Applicants were free to choose these shadow models and/or generate their own if needed in developing their MIAs.

We hosted the competition tasks as separate competitions on CodaBench.

## Evaluation Criteria

Submissions were evaluated and ranked based on their true positive rate at a 10% false positive rate (TPR @ 10% FPR). This metric reflects a realistic attack scenario in which an adversary aims to accurately identify as many members as possible while allowing only a small margin for error. We also plotted full Receiver Operating Characteristic (ROC) curves for each attack and reported additional metrics, including the Area Under the Curve (AUC), overall accuracy, and membership inference advantage (defined as TPR - FPR).

## Accessibility

We structured the competition to ensure accessibility, minimizing the need for extensive computational resources. To support this, we provided a calibrated set of shadow models so that participants were not required to train additional models themselves. For context, training the 450 models made available during the competition required approximately 1500 GPU hours. Participants were welcome to join any subset of the four competition tracks.

## Transparency

The implementations for this competition are based on the [Diffusion Model Bootcamp](https://github.com/VectorInstitute/diffusion_model_bootcamp/tree/main) provided by the Vector Institute. A more detailed technical description of the competition as well as the code used to train models and score submissions is available on the competition GitHub repository.

## Result
We received entries from 71 distinct participants across the 4 tracks. We congratulate all participants for taking part in this competition, and we are particularly excited to announce the winner and runner-up in each track.

| Track                    | Winner                                                                                     | Runner-up                                                                 |
|--------------------------|--------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| Black-box Single Table   | [Tartan Federer](https://github.com/Nicholas0228/Tartan_Federer_MIDST/tree/main)          | [CITADEL & UQAM](https://github.com/CRCHUM-CITADEL/ensemble-mia)         |
| White-box Single Table   | [Tartan Federer](https://github.com/Nicholas0228/Tartan_Federer_MIDST/tree/main)          | [Yan Pang](https://github.com/py85252876/MIDST)                           |
| Black-box Multi Table    | [Tartan Federer](https://github.com/Nicholas0228/Tartan_Federer_MIDST/tree/main)          | [Cyber@BGUlt](https://github.com/eyalgerman/MIA-EPT)                      |
| White-box Multi Table    | [Tartan Federer](https://github.com/Nicholas0228/Tartan_Federer_MIDST/tree/main)          | **                                                                        |

The winner of each track is eligible for an award from Vector of $2000 CAD; runners-up are eligible for an award of $1000 CAD.

** We received several submissions for the white-box multi-table task; however, their performance did not significantly exceed that of random guessing.

## Analysis

## Event Organizers 
[Meet the Event Organizers]({{ '/organizers/' | relative_url }})

## Event Sponsors 
[Meet the Event Sponsors]({{ '/sponsors/' | relative_url }})

## FAQ 
[Browse FAQ]({{ '/faq/' | relative_url }})

## Acknowledgements  
We'd like to thank [MICO](https://github.com/microsoft/MICO) organizers, for their open source project, and very helpful comments. 

## Contact
For more information or help with navigating our repository, please contact masoumeh@vectorinstitute.ai, xi.he@vectorinstitute.ai, john.jewell@vectorinstitute.ai or mahshid.alinoori@vectorinstitute.ai .
