{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models\n",
    "\n",
    "Recent tabular data synthesis research has focused on single tables, while real-world applications often involve complex, interconnected tables. Existing methods for multi-relational data synthesis struggle with scalability and long-range dependencies. This paper introduces Cluster Latent Variable guided Denoising Diffusion Probabilistic Models (ClavaDDPM), using clustering labels to model inter-table relationships, particularly foreign key constraints. ClavaDDPM efficiently propagates latent variables across tables, capturing long-range dependencies. Evaluations show ClavaDDPM outperforms existing methods on multi-table data and remains competitive for single-table data.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Load Configuration]()\n",
    "\n",
    "\n",
    "3. [Data Loading and Preprocessing]()\n",
    "    \n",
    "    \n",
    "4. [ClavaDDPM Algorithm]()\n",
    "\n",
    "    4.1. [Overview]()\n",
    "    \n",
    "    4.2. [Clustring]()\n",
    "    \n",
    "    4.3. [Model Training]()\n",
    "    \n",
    "    4.4. [Model Sampling]()\n",
    "    \n",
    "    \n",
    "    \n",
    "6. [Model Evaluation]()\n",
    "\n",
    "    6.1. [Multi-Table Metrics]()\n",
    "    \n",
    "    6.2. [Single-Table Metrics]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u3/k2kochar/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "from complex_pipeline import clava_clustering, clava_training, clava_load_pretrained, clava_synthesizing, clava_load_synthesized_data, clava_eval, load_configs\n",
    "from pipeline_modules import load_multi_table, load_single_table\n",
    "from gen_single_report import gen_single_report\n",
    "from report_utils import get_multi_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configuration\n",
    "\n",
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary. We print out the entire configuration file in the code cell below and will explain the hyperparameters in more detail further down to clarify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"~/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/multi_table_synthesis/berka_preprocessed/preprocessed/train\",\n",
      "        \"exp_name\": \"berka_train\",\n",
      "        \"workspace_dir\": \"clavaDDPM_workspace/berka\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"~/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/multi_table_synthesis/berka_preprocessed/preprocessed/test\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n",
      "clavaDDPM_workspace/berka/berka_train\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = 'configs/berka.json'\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. We demonstrate the dataset's metadata and parent-child relationships to provide a clearer understanding of its structure. Following this, we perform clustering to preprocess the data, facilitating the training process for the ClavaDDPM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table name: trans, Total dataframe shape: (950688, 8), Numerical data shape: (950688, 4), Categorical data shape: (950688, 4)\n",
      "Tables: \n",
      "        trans_id  account_id  trans_date  trans_type  operation  amount  \\\n",
      "0         695247        2378           0           0          3   700.0   \n",
      "1         171812         576           0           0          3   900.0   \n",
      "2         207264         704           0           0          3  1000.0   \n",
      "3        1117247        3818           0           0          3   600.0   \n",
      "4         579373        1972           1           0          3   400.0   \n",
      "...          ...         ...         ...         ...        ...     ...   \n",
      "950683   3625433        2870        2190           0          0   188.9   \n",
      "950684   3627616        2935        2190           0          0    81.3   \n",
      "950685   3625403        2869        2190           0          0    60.2   \n",
      "950686   3626683        2907        2190           0          0   107.5   \n",
      "950687   3626540        2902        2190           0          0   164.1   \n",
      "\n",
      "        balance  k_symbol  bank  account  \n",
      "0         700.0         1     0        0  \n",
      "1         900.0         1     0        0  \n",
      "2        1000.0         1     0        0  \n",
      "3         600.0         1     0        0  \n",
      "4         400.0         1     0        0  \n",
      "...         ...       ...   ...      ...  \n",
      "950683  44120.0         7     0        0  \n",
      "950684  19544.9         7     0        0  \n",
      "950685  14638.2         7     0        0  \n",
      "950686  23453.0         7     0        0  \n",
      "950687  41642.9         7     0        0  \n",
      "\n",
      "[950688 rows x 10 columns]\n",
      "\n",
      "Relation Order: \n",
      "[[None, 'trans']]\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "['trans']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load multi-table dataset\n",
    "# In this step, we load the single-table dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "# We organize the single-table dataset as a dictionary of tables, a list of relation orders, and a dictionary of dataset metadata.\n",
    "# The relation order indicates that every table is a parent\n",
    "data_dir = os.path.expanduser(configs['general']['data_dir'])\n",
    "tables, relation_order, dataset_meta = load_single_table(data_dir)\n",
    "# tables, relation_order, dataset_meta = load_multi_table(data_dir)\n",
    "\n",
    "print(\"Tables: \")\n",
    "print(tables['trans']['df'])\n",
    "print(\"\")\n",
    "print(\"Relation Order: \")\n",
    "print(relation_order)\n",
    "\n",
    "print(\"{} We show the keys of the tables dictionary below {}\".format(\"=\"*20, \"=\"*20))\n",
    "print(list(tables.keys()))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClavaDDPM Algorithm\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "<img src=\"assets/clavaDDPM.png\" alt=\"ClavaDDPM Model Pipeline\" width=\"960\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines the training process for the ClavaDDPM model. The diagram above, taken from the original paper, illustrates the main steps: \n",
    "\n",
    "(a) **Latent learning and table augmentation (steps 1-2):** This step crossponds to clustering section, where we aim to augmente each table with associated clustering labels that used to capture inter-table relationships.\n",
    "\n",
    "(b) **Training (steps 3-5):** This step corresponds to the model training section, where we train separate conditional diffusion models and the cluster classifier models on each augmented table.\n",
    "\n",
    "(c) **Synthesis (steps 6-8):** This step corresponds to the model sampling section, where we sample the table size and generate data based on the parent-child constraints (i.e., relation order).\n",
    "\n",
    "We will implement and demonstrate each section below, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "To get started, this paper first introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. Specifically, [Gaussian Process Latent Variable Models (GPLVM)](https://pyro.ai/examples/gplvm.html) are used to discover low-dimensional manifolds in noisy, high-dimensional spaces. We run the clustering algorithm below to preprocess the data for training the ClavaDDPM model. Additionally, we empirically determine the distribution of table sizes in the dataset, which will be used in the later sampling process.\n",
    "\n",
    "Mathematically, let $x$ and $y$ denote the child and parent tables, respectively. We consider $k$ clusters and model the distribution of $h = (x; \\lambda y)$ with a Gaussian distribution around its corresponding centroid $c$, i.e.,\n",
    "$$\n",
    "P(h) = \\sum_{c=1}^{k} P(c) P(h \\mid c) = \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(h; \\mu_c, \\Sigma_c),\n",
    "$$\n",
    "where the coefficient $\\lambda$ is the reweighting term called the parent scale. We opt for diagonal covariance, i.e., $\\Sigma_c = \\operatorname{diag}(\\ldots, \\sigma_l^2, \\ldots)$, which, when properly optimized, immediately satisfies our assumptions that the foreign key groups are conditionally independent of their parent rows given the cluster. \n",
    "\n",
    "A summary of the important parameters for the clustering step includes:\n",
    "- `parent_scale`: reweighting coefficient $\\lambda$ for the parent table. The default value is 1.0. It is not a sensitive factor. Recommended range for tuning: 1.0 to 2.\n",
    "- `num_clusters`: the number of clustering centers $k$ for child tables. The default is 20. Too few or too many clusters may compromise performance. Recommended range for tuning: 10 to 50.\n",
    "- `clustering_method`: ClavaDDPM provides two ways to initialize GMM-based clustering. `gmm` uses `kmeans` for initialization, and the default method `both` uses `k-means++`. It is recommended to use `both`.\n",
    "\n",
    "We expect the clava_cluster function to return following outputs:\n",
    "- `tables`: containing the updated relational tables with data augmentation, where the latent variable is attached to the parent tables.\n",
    "- `all_group_lengths_prob_dicts`: which is a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "\n",
      "Clustering checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Display important clustering parameters\n",
    "params_clustering = configs['clustering']\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\"*20, \"=\"*20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset (Works for Single-Table)\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the clustering results set, we can proceed to launch the training in PyTorch. As the dataset contains multiple tables, we train separate conditional diffusion models and cluster classifier models on each augmented table. Important parameters for the training process include:\n",
    "\n",
    "- `d_layers`: the dimension of layers in the diffusion model. \n",
    "- `num_timesteps`: the number of diffusion steps for adding noise and denoising. \n",
    "- `iterations`: the number of training iterations. The default is 10000. Recommended range for tuning: 5000 to 20000.\n",
    "- `batch_size`: the batch size for training. The default is 4096. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "d_layers: [512, 1024, 1024, 1024, 1024, 512]\n",
      "dropout: 0.0\n",
      "num_timesteps: 2000\n",
      "model_type: mlp\n",
      "iterations: 200000\n",
      "batch_size: 4096\n",
      "lr: 0.0006\n",
      "gaussian_loss_type: mse\n",
      "weight_decay: 1e-05\n",
      "scheduler: cosine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs['diffusion']\n",
    "print(\"{} We show the important sampling parameters below {}\".format(\"=\"*20, \"=\"*20))\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training from Scratch\n",
    "The training process is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. Various callbacks are configured to monitor and save the model during training. The training process is then initiated, logging progress and completing the model's training. Finally, the trained models are saved to the specified directory and returned for further use. This process is happening in the `train_model` function, which gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the training process.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the trained models and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the relation order again, each line indicates one conditional generative model ====================\n",
      "Relation 0: None ---> trans\n",
      "\n",
      "Training None -> trans model from scratch\n",
      "Model params: {'num_classes': 0, 'is_y_cond': 'none', 'rtdl_params': {'d_layers': [512, 1024, 1024, 1024, 1024, 512], 'dropout': 0.0}, 'd_in': 8}\n",
      "mlp\n",
      "Step 500/200000 MLoss: 0.0 GLoss: 0.2531 Sum: 0.2531\n",
      "Step 1000/200000 MLoss: 0.0 GLoss: 0.2379 Sum: 0.2379\n",
      "Step 1500/200000 MLoss: 0.0 GLoss: 0.232 Sum: 0.232\n",
      "Step 2000/200000 MLoss: 0.0 GLoss: 0.2309 Sum: 0.2309\n",
      "Step 2500/200000 MLoss: 0.0 GLoss: 0.2293 Sum: 0.2293\n",
      "Step 3000/200000 MLoss: 0.0 GLoss: 0.2263 Sum: 0.2263\n",
      "Step 3500/200000 MLoss: 0.0 GLoss: 0.2272 Sum: 0.2272\n",
      "Step 4000/200000 MLoss: 0.0 GLoss: 0.2253 Sum: 0.2253\n",
      "Step 4500/200000 MLoss: 0.0 GLoss: 0.2243 Sum: 0.2243\n",
      "Step 5000/200000 MLoss: 0.0 GLoss: 0.2252 Sum: 0.2252\n",
      "Step 5500/200000 MLoss: 0.0 GLoss: 0.2242 Sum: 0.2242\n",
      "Step 6000/200000 MLoss: 0.0 GLoss: 0.2239 Sum: 0.2239\n",
      "Step 6500/200000 MLoss: 0.0 GLoss: 0.2237 Sum: 0.2237\n",
      "Step 7000/200000 MLoss: 0.0 GLoss: 0.2232 Sum: 0.2232\n",
      "Step 7500/200000 MLoss: 0.0 GLoss: 0.2237 Sum: 0.2237\n",
      "Step 8000/200000 MLoss: 0.0 GLoss: 0.224 Sum: 0.224\n",
      "Step 8500/200000 MLoss: 0.0 GLoss: 0.2229 Sum: 0.2229\n",
      "Step 9000/200000 MLoss: 0.0 GLoss: 0.2217 Sum: 0.2217\n",
      "Step 9500/200000 MLoss: 0.0 GLoss: 0.2221 Sum: 0.2221\n",
      "Step 10000/200000 MLoss: 0.0 GLoss: 0.2226 Sum: 0.2226\n",
      "Step 10500/200000 MLoss: 0.0 GLoss: 0.2213 Sum: 0.2213\n",
      "Step 11000/200000 MLoss: 0.0 GLoss: 0.2212 Sum: 0.2212\n",
      "Step 11500/200000 MLoss: 0.0 GLoss: 0.2203 Sum: 0.2203\n",
      "Step 12000/200000 MLoss: 0.0 GLoss: 0.2209 Sum: 0.2209\n",
      "Step 12500/200000 MLoss: 0.0 GLoss: 0.2211 Sum: 0.2211\n",
      "Step 13000/200000 MLoss: 0.0 GLoss: 0.2198 Sum: 0.2198\n",
      "Step 13500/200000 MLoss: 0.0 GLoss: 0.2214 Sum: 0.2214\n",
      "Step 14000/200000 MLoss: 0.0 GLoss: 0.2201 Sum: 0.2201\n",
      "Step 14500/200000 MLoss: 0.0 GLoss: 0.2199 Sum: 0.2199\n",
      "Step 15000/200000 MLoss: 0.0 GLoss: 0.219 Sum: 0.219\n",
      "Step 15500/200000 MLoss: 0.0 GLoss: 0.2197 Sum: 0.2197\n",
      "Step 16000/200000 MLoss: 0.0 GLoss: 0.2196 Sum: 0.2196\n",
      "Step 16500/200000 MLoss: 0.0 GLoss: 0.2186 Sum: 0.2186\n",
      "Step 17000/200000 MLoss: 0.0 GLoss: 0.2195 Sum: 0.2195\n",
      "Step 17500/200000 MLoss: 0.0 GLoss: 0.2179 Sum: 0.2179\n",
      "Step 18000/200000 MLoss: 0.0 GLoss: 0.2196 Sum: 0.2196\n",
      "Step 18500/200000 MLoss: 0.0 GLoss: 0.2187 Sum: 0.2187\n",
      "Step 19000/200000 MLoss: 0.0 GLoss: 0.2186 Sum: 0.2186\n",
      "Step 19500/200000 MLoss: 0.0 GLoss: 0.2176 Sum: 0.2176\n",
      "Step 20000/200000 MLoss: 0.0 GLoss: 0.2191 Sum: 0.2191\n",
      "Step 20500/200000 MLoss: 0.0 GLoss: 0.2184 Sum: 0.2184\n",
      "Step 21000/200000 MLoss: 0.0 GLoss: 0.2176 Sum: 0.2176\n",
      "Step 21500/200000 MLoss: 0.0 GLoss: 0.2176 Sum: 0.2176\n",
      "Step 22000/200000 MLoss: 0.0 GLoss: 0.218 Sum: 0.218\n",
      "Step 22500/200000 MLoss: 0.0 GLoss: 0.2186 Sum: 0.2186\n",
      "Step 23000/200000 MLoss: 0.0 GLoss: 0.2172 Sum: 0.2172\n",
      "Step 23500/200000 MLoss: 0.0 GLoss: 0.2179 Sum: 0.2179\n",
      "Step 24000/200000 MLoss: 0.0 GLoss: 0.2179 Sum: 0.2179\n",
      "Step 24500/200000 MLoss: 0.0 GLoss: 0.2178 Sum: 0.2178\n",
      "Step 25000/200000 MLoss: 0.0 GLoss: 0.218 Sum: 0.218\n",
      "Step 25500/200000 MLoss: 0.0 GLoss: 0.2171 Sum: 0.2171\n",
      "Step 26000/200000 MLoss: 0.0 GLoss: 0.2175 Sum: 0.2175\n",
      "Step 26500/200000 MLoss: 0.0 GLoss: 0.2177 Sum: 0.2177\n",
      "Step 27000/200000 MLoss: 0.0 GLoss: 0.2174 Sum: 0.2174\n",
      "Step 27500/200000 MLoss: 0.0 GLoss: 0.217 Sum: 0.217\n",
      "Step 28000/200000 MLoss: 0.0 GLoss: 0.2173 Sum: 0.2173\n",
      "Step 28500/200000 MLoss: 0.0 GLoss: 0.2167 Sum: 0.2167\n",
      "Step 29000/200000 MLoss: 0.0 GLoss: 0.2169 Sum: 0.2169\n",
      "Step 29500/200000 MLoss: 0.0 GLoss: 0.2164 Sum: 0.2164\n",
      "Step 30000/200000 MLoss: 0.0 GLoss: 0.2168 Sum: 0.2168\n",
      "Step 30500/200000 MLoss: 0.0 GLoss: 0.2172 Sum: 0.2172\n",
      "Step 31000/200000 MLoss: 0.0 GLoss: 0.2165 Sum: 0.2165\n",
      "Step 31500/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 32000/200000 MLoss: 0.0 GLoss: 0.2176 Sum: 0.2176\n",
      "Step 32500/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 33000/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 33500/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 34000/200000 MLoss: 0.0 GLoss: 0.2165 Sum: 0.2165\n",
      "Step 34500/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 35000/200000 MLoss: 0.0 GLoss: 0.2169 Sum: 0.2169\n",
      "Step 35500/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 36000/200000 MLoss: 0.0 GLoss: 0.2162 Sum: 0.2162\n",
      "Step 36500/200000 MLoss: 0.0 GLoss: 0.216 Sum: 0.216\n",
      "Step 37000/200000 MLoss: 0.0 GLoss: 0.2163 Sum: 0.2163\n",
      "Step 37500/200000 MLoss: 0.0 GLoss: 0.2154 Sum: 0.2154\n",
      "Step 38000/200000 MLoss: 0.0 GLoss: 0.2167 Sum: 0.2167\n",
      "Step 38500/200000 MLoss: 0.0 GLoss: 0.2158 Sum: 0.2158\n",
      "Step 39000/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 39500/200000 MLoss: 0.0 GLoss: 0.2163 Sum: 0.2163\n",
      "Step 40000/200000 MLoss: 0.0 GLoss: 0.2162 Sum: 0.2162\n",
      "Step 40500/200000 MLoss: 0.0 GLoss: 0.2167 Sum: 0.2167\n",
      "Step 41000/200000 MLoss: 0.0 GLoss: 0.2168 Sum: 0.2168\n",
      "Step 41500/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 42000/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 42500/200000 MLoss: 0.0 GLoss: 0.2158 Sum: 0.2158\n",
      "Step 43000/200000 MLoss: 0.0 GLoss: 0.2162 Sum: 0.2162\n",
      "Step 43500/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 44000/200000 MLoss: 0.0 GLoss: 0.2168 Sum: 0.2168\n",
      "Step 44500/200000 MLoss: 0.0 GLoss: 0.2157 Sum: 0.2157\n",
      "Step 45000/200000 MLoss: 0.0 GLoss: 0.2164 Sum: 0.2164\n",
      "Step 45500/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 46000/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 46500/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 47000/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 47500/200000 MLoss: 0.0 GLoss: 0.2162 Sum: 0.2162\n",
      "Step 48000/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 48500/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 49000/200000 MLoss: 0.0 GLoss: 0.2157 Sum: 0.2157\n",
      "Step 49500/200000 MLoss: 0.0 GLoss: 0.2149 Sum: 0.2149\n",
      "Step 50000/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 50500/200000 MLoss: 0.0 GLoss: 0.2161 Sum: 0.2161\n",
      "Step 51000/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 51500/200000 MLoss: 0.0 GLoss: 0.215 Sum: 0.215\n",
      "Step 52000/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 52500/200000 MLoss: 0.0 GLoss: 0.215 Sum: 0.215\n",
      "Step 53000/200000 MLoss: 0.0 GLoss: 0.2144 Sum: 0.2144\n",
      "Step 53500/200000 MLoss: 0.0 GLoss: 0.215 Sum: 0.215\n",
      "Step 54000/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 54500/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 55000/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 55500/200000 MLoss: 0.0 GLoss: 0.2156 Sum: 0.2156\n",
      "Step 56000/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 56500/200000 MLoss: 0.0 GLoss: 0.2152 Sum: 0.2152\n",
      "Step 57000/200000 MLoss: 0.0 GLoss: 0.2151 Sum: 0.2151\n",
      "Step 57500/200000 MLoss: 0.0 GLoss: 0.2151 Sum: 0.2151\n",
      "Step 58000/200000 MLoss: 0.0 GLoss: 0.2159 Sum: 0.2159\n",
      "Step 58500/200000 MLoss: 0.0 GLoss: 0.2148 Sum: 0.2148\n",
      "Step 59000/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 59500/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 60000/200000 MLoss: 0.0 GLoss: 0.2149 Sum: 0.2149\n",
      "Step 60500/200000 MLoss: 0.0 GLoss: 0.2146 Sum: 0.2146\n",
      "Step 61000/200000 MLoss: 0.0 GLoss: 0.2158 Sum: 0.2158\n",
      "Step 61500/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 62000/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 62500/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 63000/200000 MLoss: 0.0 GLoss: 0.2151 Sum: 0.2151\n",
      "Step 63500/200000 MLoss: 0.0 GLoss: 0.2148 Sum: 0.2148\n",
      "Step 64000/200000 MLoss: 0.0 GLoss: 0.2151 Sum: 0.2151\n",
      "Step 64500/200000 MLoss: 0.0 GLoss: 0.2149 Sum: 0.2149\n",
      "Step 65000/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 65500/200000 MLoss: 0.0 GLoss: 0.2148 Sum: 0.2148\n",
      "Step 66000/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 66500/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 67000/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 67500/200000 MLoss: 0.0 GLoss: 0.2148 Sum: 0.2148\n",
      "Step 68000/200000 MLoss: 0.0 GLoss: 0.2148 Sum: 0.2148\n",
      "Step 68500/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 69000/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 69500/200000 MLoss: 0.0 GLoss: 0.215 Sum: 0.215\n",
      "Step 70000/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 70500/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 71000/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 71500/200000 MLoss: 0.0 GLoss: 0.2144 Sum: 0.2144\n",
      "Step 72000/200000 MLoss: 0.0 GLoss: 0.2147 Sum: 0.2147\n",
      "Step 72500/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 73000/200000 MLoss: 0.0 GLoss: 0.2153 Sum: 0.2153\n",
      "Step 73500/200000 MLoss: 0.0 GLoss: 0.2152 Sum: 0.2152\n",
      "Step 74000/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 74500/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 75000/200000 MLoss: 0.0 GLoss: 0.2149 Sum: 0.2149\n",
      "Step 75500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 76000/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 76500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 77000/200000 MLoss: 0.0 GLoss: 0.2146 Sum: 0.2146\n",
      "Step 77500/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 78000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 78500/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 79000/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 79500/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 80000/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 80500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 81000/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 81500/200000 MLoss: 0.0 GLoss: 0.2144 Sum: 0.2144\n",
      "Step 82000/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 82500/200000 MLoss: 0.0 GLoss: 0.215 Sum: 0.215\n",
      "Step 83000/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 83500/200000 MLoss: 0.0 GLoss: 0.2146 Sum: 0.2146\n",
      "Step 84000/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 84500/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 85000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 85500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 86000/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 86500/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 87000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 87500/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 88000/200000 MLoss: 0.0 GLoss: 0.2147 Sum: 0.2147\n",
      "Step 88500/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 89000/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 89500/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 90000/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 90500/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 91000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 91500/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 92000/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 92500/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 93000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 93500/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 94000/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 94500/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 95000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 95500/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 96000/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 96500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 97000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 97500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 98000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 98500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 99000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 99500/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 100000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 100500/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 101000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 101500/200000 MLoss: 0.0 GLoss: 0.2142 Sum: 0.2142\n",
      "Step 102000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 102500/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 103000/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 103500/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 104000/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 104500/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 105000/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 105500/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 106000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 106500/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 107000/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 107500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 108000/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 108500/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 109000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 109500/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 110000/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 110500/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 111000/200000 MLoss: 0.0 GLoss: 0.2139 Sum: 0.2139\n",
      "Step 111500/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 112000/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 112500/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 113000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 113500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 114000/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 114500/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 115000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 115500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 116000/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 116500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 117000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 117500/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 118000/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 118500/200000 MLoss: 0.0 GLoss: 0.2135 Sum: 0.2135\n",
      "Step 119000/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 119500/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 120000/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 120500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 121000/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 121500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 122000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 122500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 123000/200000 MLoss: 0.0 GLoss: 0.2136 Sum: 0.2136\n",
      "Step 123500/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 124000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 124500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 125000/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 125500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 126000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 126500/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 127000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 127500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 128000/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 128500/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 129000/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 129500/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 130000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 130500/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 131000/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 131500/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 132000/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 132500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 133000/200000 MLoss: 0.0 GLoss: 0.2127 Sum: 0.2127\n",
      "Step 133500/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 134000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 134500/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 135000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 135500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 136000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 136500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 137000/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 137500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 138000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 138500/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 139000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 139500/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 140000/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 140500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 141000/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 141500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 142000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 142500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 143000/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 143500/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 144000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 144500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 145000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 145500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 146000/200000 MLoss: 0.0 GLoss: 0.213 Sum: 0.213\n",
      "Step 146500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 147000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 147500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 148000/200000 MLoss: 0.0 GLoss: 0.2127 Sum: 0.2127\n",
      "Step 148500/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 149000/200000 MLoss: 0.0 GLoss: 0.2127 Sum: 0.2127\n",
      "Step 149500/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 150000/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 150500/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 151000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 151500/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 152000/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 152500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 153000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 153500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 154000/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 154500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 155000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 155500/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 156000/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 156500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 157000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 157500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 158000/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 158500/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 159000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 159500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 160000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 160500/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 161000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 161500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 162000/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 162500/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 163000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 163500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 164000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 164500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 165000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 165500/200000 MLoss: 0.0 GLoss: 0.2108 Sum: 0.2108\n",
      "Step 166000/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 166500/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 167000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 167500/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 168000/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 168500/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 169000/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 169500/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 170000/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 170500/200000 MLoss: 0.0 GLoss: 0.2113 Sum: 0.2113\n",
      "Step 171000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 171500/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 172000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 172500/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 173000/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 173500/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 174000/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 174500/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 175000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 175500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 176000/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 176500/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 177000/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 177500/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 178000/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 178500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 179000/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 179500/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 180000/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 180500/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 181000/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 181500/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 182000/200000 MLoss: 0.0 GLoss: 0.2111 Sum: 0.2111\n",
      "Step 182500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 183000/200000 MLoss: 0.0 GLoss: 0.211 Sum: 0.211\n",
      "Step 183500/200000 MLoss: 0.0 GLoss: 0.2113 Sum: 0.2113\n",
      "Step 184000/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 184500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 185000/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 185500/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 186000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 186500/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 187000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 187500/200000 MLoss: 0.0 GLoss: 0.2122 Sum: 0.2122\n",
      "Step 188000/200000 MLoss: 0.0 GLoss: 0.2111 Sum: 0.2111\n",
      "Step 188500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 189000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 189500/200000 MLoss: 0.0 GLoss: 0.2113 Sum: 0.2113\n",
      "Step 190000/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 190500/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 191000/200000 MLoss: 0.0 GLoss: 0.2112 Sum: 0.2112\n",
      "Step 191500/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 192000/200000 MLoss: 0.0 GLoss: 0.2121 Sum: 0.2121\n",
      "Step 192500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 193000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 193500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 194000/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 194500/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 195000/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 195500/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 196000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 196500/200000 MLoss: 0.0 GLoss: 0.211 Sum: 0.211\n",
      "Step 197000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 197500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 198000/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 198500/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 199000/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 199500/200000 MLoss: 0.0 GLoss: 0.2112 Sum: 0.2112\n",
      "Step 200000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n"
     ]
    }
   ],
   "source": [
    "# Relation order is the topological order of the multi-table dataset\n",
    "print(\"{} We show the relation order again, each line indicates one conditional generative model {}\".format(\"=\"*20, \"=\"*20))\n",
    "for i, (from_item, to_item) in enumerate(relation_order):\n",
    "    print(\"Relation {}: {} ---> {}\".format(i, from_item, to_item))\n",
    "print(\"\")\n",
    "\n",
    "# Launch training from scratch\n",
    "models = clava_training(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models\n",
    "If the training process from scratch takes too long, please run the following command to load pre-trained models and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -> trans checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Use the pre-trained models\n",
    "# pretrained_dir = \"/fs01/projects/diffusion_bootcamp/models/tabular/clavaDDPM/movie_lens_pretrained/movie_lens_train\"\n",
    "\n",
    "# There is now a pkl file in this path under models/ (Should be the pretrained model)\n",
    "pretrained_dir = \"~/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/multi_table_synthesis/clavaDDPM_workspace/berka/berka_train\"\n",
    "pretrained_dir = os.path.expanduser(pretrained_dir)\n",
    "pretrained_models = clava_load_pretrained(relation_order, pretrained_dir)\n",
    "\n",
    "# Uncomment this to use the pretrained_model from the .pkl file\n",
    "# models = pretrained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling\n",
    "\n",
    "To assess the trained model's performance, we generate synthetic samples and showcase the results qualitatively. We initiate the generation process by sampling the table size (i.e., the number of rows per table) and performing conditional generation to meet the parent-child constraints (i.e., relation order). Quantitative evaluations will be conducted in the next section.\n",
    "\n",
    "Important parameters for the sampling process include:\n",
    "- `batch_size`: Mini-batch size for sampling.\n",
    "- `classifier_scale` ($\\eta$): Controls the magnitude of classifier gradients during guided sampling, balancing sample quality and conditional sampling accuracy. The default value is 1.0. When $\\eta = 0$ (disabling classifier conditioning), single column densities (1-way) may improve but fail to capture long-range correlations. When $\\eta = 2$, the increased conditioning weight significantly improves the modeling of multi-hop correlations compared to $\\eta = 0$. Recommended tuning range: 0 to 2.\n",
    "\n",
    "<!-- We also conduct a matching process to determine the parent-child table relationship of the generated data. This process uses an approximate nearest neighbor search-based matching technique, providing a universal solution to the multi-parent relational synthesis problem for a child table with multiple parents.\n",
    "\n",
    "Important parameters are as follows:\n",
    "- `num_matching_clusters`: Number of clusters used in the matching process.\n",
    "- `matching_batch_size`: Mini-batch size for table matching.\n",
    "- `unique_matching`: Boolean flag indicating if the matching result should be unique.\n",
    "- `no_matching`: Boolean flag to disable the matching process. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "batch_size: 20000\n",
      "classifier_scale: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs['sampling']\n",
    "print(\"{} We show the important sampling parameters below {}\".format(\"=\"*20, \"=\"*20))\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data from Scratch\n",
    "To generate synthetic data from scratch, we run the following code cell. This `clava_synthesizing` function gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the synthetic data.\n",
    "- `all_group_lengths_prob_dicts`: a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate.\n",
    "- `models`: the trained diffusion models.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the sampling process.\n",
    "- `sample_scale`: the scale factor for the sampling process.\n",
    "\n",
    "The synthetic data will be saved in the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Currently, have not run or tested anything beyond the prev cell\n",
    "# Everything before this point should work normally before this for single-table ClavaDDPM\n",
    "# The cells after this have not been verified\n",
    "\n",
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables, \n",
    "    relation_order, \n",
    "    save_dir, \n",
    "    all_group_lengths_prob_dicts, \n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if not 'debug' in configs else configs['debug']['sample_scale']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as some integer values are saved as strings during this process, we convert them back to integers for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == 'object':\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-Synthesized Data\n",
    "If the generation process takes too long, please run the following command to load pre-synthesized data. We provide the pre-synthesized data using the aforementioned pre-trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-synthesized data\n",
    "pretrained_dir = \"/fs01/projects/diffusion_bootcamp/models/tabular/clavaDDPM/movie_lens_pretrained/movie_lens_train\"\n",
    "cleaned_tables = clava_load_synthesized_data(tables.keys(), pretrained_dir)\n",
    "\n",
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == 'object':\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Table Metrics\n",
    "In this step, we quantitatively evaluate the generated tabular data by computing metrics to determine the accuracy of the predictions, specifically assessing how closely the generated data matches the observed samples in the reference dataset.\n",
    "\n",
    "In particular, the critical multi-table metrics are as follows:\n",
    "\n",
    "1. Pair-wise column correlation (k-hop): This metric measures the correlations between columns from tables at a distance k (e.g., 0-hop for columns within the same table, 1-hop for a column and a column from its parent or child table).\n",
    "\n",
    "2. Average 2-way: This metric computes the average of all k-hop column-pair correlations, taking into account both short-range (k = 0) and longer-range (k > 0) dependencies.\n",
    "\n",
    "We use the [SDV evaluation API](https://docs.sdv.dev/sdv/multi-table-data/evaluation/data-quality) to obtain these metrics. For more details about the computation process, refer to their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-table Evaluation\n",
    "report = clava_eval(tables, save_dir, configs, relation_order, cleaned_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the multi-table metrics\n",
    "n_rows = 3\n",
    "for key, val in report.items():\n",
    "    if key in ['hop_relation', 'avg_scores', 'all_avg_score']:\n",
    "        if key == 'hop_relation':\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                if k > 0:\n",
    "                    print(\"{:20}-hop column correlation, format '(Parent Table, Child Table, Column 1, Column 2): Correlation score'\".format(k))\n",
    "                else:\n",
    "                    print(\"{:20}-hop column correlation, format '(Parent Table, Parent Table, Column 1, Column 2): Correlation score'\".format(k))\n",
    "                for i_row, (k2, v2) in enumerate(v.items()):\n",
    "                    if i_row < n_rows:\n",
    "                        print(\"{:20} {}: {}\".format('', k2, v2))\n",
    "                print(\"{:20} ...... other rows are omitted ......\\n\".format(''))\n",
    "        elif key == 'avg_scores':\n",
    "            print(\"{} metrics:\".format(key))\n",
    "            for k, v in val.items():\n",
    "                print(\"{:20}-hop column correlation: {}\".format(k, v))\n",
    "        elif key == 'all_avg_score':\n",
    "            print(\"{:20}: {}\".format(key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Table Metrics\n",
    "While here we study multi-table metrics, we also provide a notebook to evaluate single table metrics for each synthesized table. Please refer to `single_table_synthesis/evalutate_synthetic_data.ipynb` for more details. \n",
    "\n",
    "<!-- The key metrics are as follows:\n",
    "To study single table metrics please refer to `single_table_synthesis/evalutate_synthetic_data.ipynb` notebook. We highlight some of the key metrics below:\n",
    "- Single Column Similarity Score: Compares the density distributions of individual columns in real and synthetic data to measure similarity.\n",
    "\n",
    "- Pair-wise Correlation Score: Evaluates the preservation of relationships between pairs of columns, using correlation or contingency similarity.\n",
    "\n",
    "- alpha-Precision and beta-Recall: Precision assesses the quality of synthetic data by how closely it matches real data points. Recall measures the diversity by how well synthetic data covers the variability of real data.\n",
    "\n",
    "- Privacy Protection: Distance to Closest Record (DCR): Ensures synthetic data points are sufficiently distant from real data points to prevent privacy leakage.\n",
    "\n",
    "- Detection: Classifier Two Sample Tests (C2ST): Measures the realism of synthetic data by evaluating if a machine learning model can distinguish it from real data. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Prepare the synthetic data and reference data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(save_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    # uncomment and run the following line if you want to use the pre-synthesized data\n",
    "    # shutil.copy(os.path.join(pretrained_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir, verbose=False)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'], verbose=False)\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
